\documentclass[main.tex]{subfiles}
\begin{document}


\subsection{Adaptation code}
In this section, the adaptation code is described that was written to execute the test suites generated by the MTR tool for the Kubernetes pod lifecycle model. The purpose of the adaptation code is to interface with the Kubernetes cluster using `kubectl ` commands and to monitor pod states based on the inputs from the test suite and the expected results.

To parse the generated test suites, a JSON test suite parser was written. It returns the input and the output symbols in a list, so that the test runner class can easily iterate through them and execute each transition. 
The adaptation code contains two important mappings, which also contain the main logic of the program:
\begin{itemize}
\item \textbf{Input Mappers}: The input mapper maps the current input symbol to the corresponding function call in the program. This is where it is decided which Kubernetes action needs to be taken (e.g., creating or deleting a Pod). The input is mapped to the relevant functions that handle the Kubernetes API interactions through `kubectl`. Each transition was performed with \texttt{kubectl apply -f} command, where different Pod definitions were provided for the different test scenarios. For example to test a Pod getting into the ImagePullBackOff state, the Pod definition intentionally contained a faulty image name. 

\textit{Notes on limitations: Since it is not permitted by the Kubernetes API to modify certain properties of a Pod such as Liveness probes while running, it was only possible to carry out each transitions if every time the Pod was deleted and then recreated with a new Pod definition. This actually caused longer runtime, especially because before some actions like asserting that the pull of a Docker image was successfull it was necessary to include some waiting times.} 

\item \textbf{Output Mappers}: Since the FSM model was created in a way that the output symbols correspond to the next state of the model, these could be used for assertions too. For this, the output mapper translates the states into assertions, where we check the Pod states after each transitions. So in the end, the result of these checks are compared to the current expected output symbol of the generated test suite.
\end{itemize}


In summary, the adaptation code works as follows:
\begin{itemize}
    \item \textbf{Test Suite Parsing}: The test suite JSON is parsed to extract the input and expected output list.
    \item \textbf{Kubernetes Interaction}: Using `kubectl`, the code performs the corresponding actions (e.g., creating a Pod with different definitions, deleting a Pod).
    \item \textbf{State Monitoring}: The code monitors the pod's state after each operation and compares it against the expected output.
\end{itemize}


\subsection{Prerequisites}
The adaptation code was developed and executed on a Windows machine with the following setup:

\begin{itemize}
    \item \textbf{Docker Desktop}: Version 4.19.0 (or the latest version available) - used for running a local Kubernetes cluster in conjunction with Minikube.
    \item \textbf{Minikube}: Version 1.30.0 (or the latest version available) - used to create and manage a local Kubernetes cluster.
    \item \textbf{kubectl}: Version 1.25.0 (or the latest version available) - the Kubernetes command-line tool used to interact with the Kubernetes API for managing pods, deployments, etc.
    \item \textbf{Java}: Version 17 (or the most recent version of the OpenJDK) - the programming language used for writing the adaptation code and handling the test execution.
\end{itemize}
For being able to run the test, the above mentioned tools are needed, and the Docker Desktop and Minikube must be started.

\subsection{Test Execution Report}
The test execution phase was carried out using the adaptation code, which interfaces with the Kubernetes cluster and executedthe generated test suites. The results of these executions, including any errors or issues encountered, are summarized in the following sections.

All of the test suites were executed that was generated by the algorithms described in the previous chapter:
Random walk (100\% state coverage), All State, Transition Tour, All Transition State, N-switch coverage. Neither of them discovered any fault, all test passed. The differences were only the test execution times, which were expected because of the various test sequence lengths. Furthermore, as mentioned above, some transitions like Docker image pull, or waiting for a Pod to become ready required more waiting time, so the test suite that used more of these types of transitions were naturally longer to execute.
The following table summarizes the execution times:
\begin{table}[h!]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Algorithm}     & \textbf{Elapsed Time} & \textbf{Test sequence length}\\ \hline
Random Walk (100\% state c.)     & 232 s & 34 \\ \hline
All-State     & 141 s & 18 \\ \hline
Transition Tour     & 330 s    & 54 \\ \hline
All-Transition-State     & 1510 s  & 193\\ \hline
N-Switch Coverage (N=1)     &  1523 s   &  200 \\ \hline
\end{tabular}
\caption{Comparison of Test Executions}
\label{tab:test_exec_comparison}
\end{table}

\subsection{Conclusion}
Based on the results of the test executions, we conclude that the Kubernetes pod lifecycle management is functioning as expected. All test cases passed, and no issues were identified during the testing phase.

\end{document}